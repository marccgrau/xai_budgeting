{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-30T07:05:31.347176Z",
     "start_time": "2024-05-30T07:05:19.743690Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "output_directory = \"../data/final\" \n",
    "file_path = os.path.join(output_directory, \"merged_complete_preprocessed.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "df_zh = df[df['Region'] == 'ZH']\n",
    "df_zh.set_index('Year', inplace=True)\n",
    "df_zh.drop('Region', axis=1, inplace=True)\n",
    "\n",
    "for column in df_zh.columns:\n",
    "    plt.figure(figsize=(10, 6))  \n",
    "    plt.plot(df_zh.index, df_zh[column], marker='o', linestyle='-', label=column)\n",
    "    plt.title(f'{column} over Time for Canton ZH (2013-2021)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(column)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output_directory = \"../data/final\" \n",
    "file_path = os.path.join(output_directory, \"merged_double_digit.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "#print uniques values for Region and Year\n",
    "print(df['Region'].unique())\n",
    "print(df['Year'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T07:05:31.365331Z",
     "start_time": "2024-05-30T07:05:31.349266Z"
    }
   },
   "id": "b2db649e65dd6d31",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_directory_final = \"../data/final\"\n",
    "file_path_double_digit = os.path.join(output_directory_final, \"merged_double_digit.csv\")\n",
    "output_directory_merged = \"../data/merged_canton\" \n",
    "file_path_kantonsdaten_merged = os.path.join(output_directory_merged, \"Kantonsdaten_2013_to_2021_merged.csv\")\n",
    "\n",
    "df_double_digit = pd.read_csv(file_path_double_digit)\n",
    "df_kantonsdaten = pd.read_csv(file_path_kantonsdaten_merged)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(df_double_digit, df_kantonsdaten, how='outer', left_on=['Region', 'Year'], right_on=['Canton', 'Year'])\n",
    "# merged_df.fillna(0, inplace=True)\n",
    "merged_df = merged_df[merged_df['Region'] != 0]\n",
    "merged_df.drop('Canton', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "merged_output_path = os.path.join(output_directory_final, \"merged_complete.csv\")\n",
    "merged_df.to_csv(merged_output_path, index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T07:05:32.141177Z",
     "start_time": "2024-05-30T07:05:31.366788Z"
    }
   },
   "id": "a5c162dc761c8618",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "merged_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T07:05:32.158878Z",
     "start_time": "2024-05-30T07:05:32.142779Z"
    }
   },
   "id": "d1e0cc3b61820815",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/final/merged_complete.csv')\n",
    "\n",
    "#delete all the rows where Realized\tBudget y\tBudget y+1\tSlack are all 0\n",
    "df = df[(df['Realized'] != 0) | (df['Budget y'] != 0) | (df['Budget y+1'] != 0) | (df['Slack'] != 0)]\n",
    "# # Exclude Year from numeric columns for preprocessing\n",
    "# exclude_columns = ['Year', 'Acc-ID', 'Realized', 'Budget y', 'Budget y+1', 'Slack']\n",
    "# \n",
    "# numeric_columns = [col for col in df.select_dtypes(include=[np.number]).columns if col not in exclude_columns]\n",
    "# \n",
    "# \n",
    "# # Apply median imputation to the selected numeric columns\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "# \n",
    "# # Detect outliers based on the selected numeric columns (don't remove yet)\n",
    "# z_scores = np.abs(stats.zscore(df[numeric_columns]))\n",
    "# outliers = (z_scores < 3).all(axis=1)\n",
    "# \n",
    "# # Keep a copy of the DataFrame with outliers removed\n",
    "# df_no_outliers = df[outliers].copy()\n",
    "# \n",
    "# # Apply scaling\n",
    "# scaler = StandardScaler()\n",
    "# df_no_outliers[numeric_columns] = scaler.fit_transform(df_no_outliers[numeric_columns])\n",
    "# \n",
    "# # Apply transformation to make data more Gaussian-like\n",
    "# transformer = PowerTransformer()\n",
    "# df_no_outliers[numeric_columns] = transformer.fit_transform(df_no_outliers[numeric_columns])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the preprocessed DataFrame with both numeric and categorical data\n",
    "df.to_csv('../data/final/merged_complete_preprocessed.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T07:05:32.443413Z",
     "start_time": "2024-05-30T07:05:32.162072Z"
    }
   },
   "id": "8943c15def0db650",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is loaded into a DataFrame named df\n",
    "df = pd.read_csv('../data/final/merged_complete.csv')\n",
    "\n",
    "# Ensure 'Year' is sorted\n",
    "df = df.sort_values(by=['Region', 'Year'])\n",
    "\n",
    "# Identify rows for the years 2011, 2012, and 2022\n",
    "years_of_interest = [2011, 2012, 2022]\n",
    "\n",
    "# Define columns that won't be interpolated\n",
    "columns_not_to_interpolate = ['Year', 'Region', 'Acc-ID', 'Realized', 'Budget y', 'Budget y+1', 'Slack']\n",
    "\n",
    "# Perform group-wise forward fill followed by backward fill for each region\n",
    "# This aims to fill missing values for the specific years where linear interpolation might not be applicable\n",
    "for region in df['Region'].unique():\n",
    "    for year in years_of_interest:\n",
    "        # Select rows for the specific region and year\n",
    "        mask = (df['Region'] == region) & (df['Year'] == year)\n",
    "        # Apply forward fill followed by backward fill within the region for the target year\n",
    "        df.loc[mask, :] = df.loc[mask, :].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Check if any missing values remain for the specified years\n",
    "missing_values_check = df[df['Year'].isin(years_of_interest)].isnull().sum()\n",
    "print(missing_values_check)\n",
    "\n",
    "df.to_csv('../data/final/merged_complete_preprocessed.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a97da1e2ead3138",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99f7dc1d96ca9a41",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/final/merged_complete.csv')\n",
    "df = df.sort_values(by=['Region', 'Year'])\n",
    "\n",
    "columns_to_fill = [col for col in df.columns if col not in ['Year', 'Region', 'Acc-ID', 'Realized', 'Budget y', 'Budget y+1', 'Slack']]\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    df[column].replace(0, np.nan, inplace=True)\n",
    "\n",
    "for region in df['Region'].unique():\n",
    "    region_df = df[df['Region'] == region]\n",
    "    \n",
    "    for column in columns_to_fill:\n",
    "        not_null_region_df = region_df.dropna(subset=[column])\n",
    "        \n",
    "        if len(not_null_region_df) > 1:\n",
    "            X = not_null_region_df[['Year']]\n",
    "            y = not_null_region_df[column]\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            missing_df = region_df[region_df[column].isnull()]\n",
    "            if not missing_df.empty:\n",
    "                predicted_values = model.predict(missing_df[['Year']])\n",
    "                \n",
    "                df.loc[missing_df.index, column] = predicted_values\n",
    "\n",
    "df = df.dropna(subset=['Region'])\n",
    "columns_to_drop = set()\n",
    "\n",
    "for region in df['Region'].unique():\n",
    "    region_df = df[df['Region'] == region]\n",
    "    \n",
    "    for column in columns_to_fill:\n",
    "        if region_df[column].isnull().all():\n",
    "            columns_to_drop.add(column)\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "columns_to_fill = [col for col in columns_to_fill if col not in columns_to_drop]\n",
    "\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "output_path = '../data/final/merged_complete_preprocessed.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame with filled values for multiple columns, excluding dropped columns, saved to {output_path}.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5560894cd7196693",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/final/merged_complete.csv')\n",
    "df = df.sort_values(by=['Region', 'Year'])\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = [col for col in df.columns if col not in ['Year', 'Region', 'Acc-ID', 'Realized', 'Budget y', 'Budget y+1', 'Slack']]\n",
    "\n",
    "# Replace 0 with NaN for specified columns\n",
    "df[columns_to_fill] = df[columns_to_fill].replace(0, np.nan)\n",
    "\n",
    "# Replace inf and -inf with NaN\n",
    "df[columns_to_fill] = df[columns_to_fill].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Iterate through each region\n",
    "for region in df['Region'].unique():\n",
    "    region_df = df[df['Region'] == region].copy()\n",
    "    region_df.set_index('Year', inplace=True)\n",
    "    \n",
    "    # Forward fill\n",
    "    region_df[columns_to_fill] = region_df[columns_to_fill].ffill()\n",
    "    \n",
    "    # Backward fill\n",
    "    region_df[columns_to_fill] = region_df[columns_to_fill].bfill()\n",
    "    \n",
    "    # Interpolate\n",
    "    region_df[columns_to_fill] = region_df[columns_to_fill].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # Assign filled data back to the original DataFrame\n",
    "    df.loc[df['Region'] == region, columns_to_fill] = region_df[columns_to_fill].values\n",
    "\n",
    "# Drop rows where 'Region' is NaN\n",
    "df = df.dropna(subset=['Region'])\n",
    "\n",
    "# Identify columns to drop if all values are NaN within a region\n",
    "columns_to_drop = set()\n",
    "\n",
    "for region in df['Region'].unique():\n",
    "    region_df = df[df['Region'] == region]\n",
    "    \n",
    "    for column in columns_to_fill:\n",
    "        if region_df[column].isnull().all():\n",
    "            columns_to_drop.add(column)\n",
    "\n",
    "# Drop identified columns\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Update the columns to fill\n",
    "columns_to_fill = [col for col in columns_to_fill if col not in columns_to_drop]\n",
    "\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "# Save the preprocessed DataFrame\n",
    "output_path = '../data/final/merged_complete_preprocessed.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame with filled values for multiple columns, excluding dropped columns, saved to {output_path}.\")"
   ],
   "id": "b3c5b46f9a1b2ad3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor  # Import Random Forest Regressor\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/final/merged_complete.csv')\n",
    "df = df.sort_values(by=['Region', 'Year'])\n",
    "\n",
    "# Define the columns you want to fill\n",
    "columns_to_fill = [col for col in df.columns if col not in ['Year', 'Region', 'Acc-ID', 'Realized', 'Budget y', 'Budget y+1', 'Slack']]\n",
    "\n",
    "# Convert 0.0 to NaN for specified columns\n",
    "for column in columns_to_fill:\n",
    "    df[column].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Initialize a set to store names of columns to drop\n",
    "\n",
    "# Proceed with the existing process, now with the updated DataFrame and columns_to_fill list\n",
    "for region in df['Region'].unique():\n",
    "    print(f\"Processing region: {region}\")\n",
    "    region_df = df[df['Region'] == region]\n",
    "    \n",
    "    for column in columns_to_fill:\n",
    "        not_null_region_df = region_df.dropna(subset=[column])\n",
    "        \n",
    "        if len(not_null_region_df) > 1:\n",
    "            X = not_null_region_df[['Year']]\n",
    "            y = not_null_region_df[column]\n",
    "            \n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)  # Use RandomForestRegressor\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            missing_df = region_df[region_df[column].isnull()]\n",
    "            if not missing_df.empty:\n",
    "                predicted_values = model.predict(missing_df[['Year']])\n",
    "                \n",
    "                df.loc[missing_df.index, column] = predicted_values\n",
    "                print(f\"Updated column '{column}' for region '{region}' with {len(predicted_values)} predicted values.\")\n",
    "            else:\n",
    "                print(f\"No missing values to update for column '{column}' in region '{region}'.\")\n",
    "        else:\n",
    "            print(f\"Not enough data to model column '{column}' in region '{region}'.\")\n",
    "\n",
    "df = df.dropna(subset=['Region'])\n",
    "columns_to_drop = set()\n",
    "\n",
    "# Loop over each region and check columns for all NaN values\n",
    "for region in df['Region'].unique():\n",
    "    region_df = df[df['Region'] == region]\n",
    "    \n",
    "    for column in columns_to_fill:\n",
    "        if region_df[column].isnull().all():\n",
    "            columns_to_drop.add(column)\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Now, columns_to_fill needs to be updated to exclude dropped columns\n",
    "columns_to_fill = [col for col in columns_to_fill if col not in columns_to_drop]\n",
    "\n",
    "# Print the names of dropped columns\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "# Save the modified DataFrame\n",
    "output_path = '../data/final/merged_complete_preprocessed.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame with filled values for multiple columns, excluding dropped columns, saved to {output_path}.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed28436beb3ba955",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../feature_importances/feature_importances.csv')\n",
    "df = df[(df['Importance'] < 0.01)]\n",
    "df = df['Feature']\n",
    "for feature in df:\n",
    "    print('\"' + feature + '\",')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a2e16c1560817a4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/final/merged_double_digit.csv')\n",
    "\n",
    "# Preprocess the DataFrame\n",
    "df = df.sort_values(by=['Region', 'Year'])\n",
    "df = df.dropna(subset=['Region', 'Year'])\n",
    "\n",
    "yearMax = 2022\n",
    "yearGeneration = 11\n",
    "years = range(yearMax - yearGeneration, yearMax)  \n",
    "yearMin = min(years)\n",
    "\n",
    "\n",
    "# Get unique combinations of Region and Acc-ID\n",
    "unique_regions_acc_ids = df[['Region', 'Acc-ID']].drop_duplicates()\n",
    "\n",
    "# Create a DataFrame with all combinations\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [unique_regions_acc_ids['Region'].unique(), unique_regions_acc_ids['Acc-ID'].unique(), years],\n",
    "    names=['Region', 'Acc-ID', 'Year']\n",
    ")\n",
    "full_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "\n",
    "# Merge with original data\n",
    "full_df = pd.merge(full_df, df, how='left', on=['Region', 'Acc-ID', 'Year'])\n",
    "\n",
    "# Save the output\n",
    "output_path = '../data/final/checking.csv'\n",
    "full_df.to_csv(output_path, index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "277ef712471c7fca",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/final/checking.csv')\n",
    "df = df.sort_values(by=['Region', 'Year', 'Acc-ID'])\n",
    "\n",
    "# Define the columns you want to fill\n",
    "columns_to_fill = [col for col in df.columns if col not in ['Year', 'Region', 'Acc-ID', 'Slack']]\n",
    "\n",
    "# Convert 0.0 to NaN for specified columns\n",
    "for column in columns_to_fill:\n",
    "    df[column].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Initialize a set to store names of columns to drop\n",
    "\n",
    "# Proceed with the existing process, now with the updated DataFrame and columns_to_fill list\n",
    "# for region in df['Region'].unique():\n",
    "#     for acc_id in df[df['Region'] == region]['Acc-ID'].unique():\n",
    "#         sub_df = df[(df['Region'] == region) & (df['Acc-ID'] == acc_id)]\n",
    "#         print(f\"Processing region: {region}, Acc-ID: {acc_id}\")\n",
    "# \n",
    "#         for column in columns_to_fill:\n",
    "#             not_null_sub_df = sub_df.dropna(subset=[column])\n",
    "#             \n",
    "#             if len(not_null_sub_df) > 1:\n",
    "#                 X = not_null_sub_df[['Year']]\n",
    "#                 y = not_null_sub_df[column]\n",
    "#                 \n",
    "#                 model = RandomForestRegressor(n_estimators=100, n_jobs=-1) \n",
    "#                 model.fit(X, y)\n",
    "#                 \n",
    "#                 missing_df = sub_df[sub_df[column].isnull()]\n",
    "#                 if not missing_df.empty:\n",
    "#                     predicted_values = model.predict(missing_df[['Year']])\n",
    "#                     \n",
    "#                     df.loc[missing_df.index, column] = predicted_values\n",
    "#                     print(f\"Updated column '{column}' for region '{region}', Acc-ID '{acc_id}' with {len(predicted_values)} predicted values.\")\n",
    "#                 else:\n",
    "#                     print(f\"No missing values to update for column '{column}' in region '{region}', Acc-ID '{acc_id}'.\")\n",
    "#             else:\n",
    "#                 print(f\"Not enough data to model column '{column}' in region '{region}', Acc-ID '{acc_id}'.\")\n",
    "\n",
    "\n",
    "for region in df['Region'].unique():\n",
    "    for acc_id in df[df['Region'] == region]['Acc-ID'].unique():\n",
    "        sub_df = df[(df['Region'] == region) & (df['Acc-ID'] == acc_id)]\n",
    "        for column in columns_to_fill:\n",
    "            not_null_sub_df = sub_df.dropna(subset=[column])\n",
    "            if len(not_null_sub_df) > 1:\n",
    "                X = not_null_sub_df[['Year']].values\n",
    "                y = not_null_sub_df[column].values\n",
    "                \n",
    "                # Set up a pipeline with PolynomialFeatures and LinearRegression\n",
    "                degree = 2  # You can adjust the degree of the polynomial based on your data and needs\n",
    "                model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "                model.fit(X, y)\n",
    "                \n",
    "                missing_df = sub_df[sub_df[column].isnull()]\n",
    "                if not missing_df.empty:\n",
    "                    predicted_values = model.predict(missing_df[['Year']].values)\n",
    "                    df.loc[missing_df.index, column] = predicted_values\n",
    "                    print(f\"Updated column '{column}' for region '{region}', Acc-ID '{acc_id}' with {len(predicted_values)} predicted values.\")\n",
    "            else:\n",
    "                print(f\"Not enough data to model column '{column}' in region '{region}', Acc-ID '{acc_id}'.\")\n",
    "                \n",
    "df = df.dropna(subset=['Region'])\n",
    "columns_to_drop = set()\n",
    "\n",
    "# Loop over each region and check columns for all NaN values\n",
    "for region in df['Region'].unique():\n",
    "    region_df = df[df['Region'] == region]\n",
    "    \n",
    "    for column in columns_to_fill:\n",
    "        if region_df[column].isnull().all():\n",
    "            columns_to_drop.add(column)\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Now, columns_to_fill needs to be updated to exclude dropped columns\n",
    "columns_to_fill = [col for col in columns_to_fill if col not in columns_to_drop]\n",
    "\n",
    "# Print the names of dropped columns\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "# Save the modified DataFrame\n",
    "output_path = '../data/final/checking2.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame with filled values for multiple columns, excluding dropped columns, saved to {output_path}.\")\n",
    "\n",
    "# Drop column called Slack\n",
    "df = df.drop('Slack', axis=1)\n",
    "#add column Slack this is calculated Budget Y - Realized\n",
    "df['Slack'] = df['Budget y'] - df['Realized']\n",
    "#drop rows which contain NaN values\n",
    "df = df.dropna()\n",
    "# Save the output\n",
    "output_path = '../data/final/checking.csv'\n",
    "\n",
    "df.to_csv(output_path + str(yearMin) + 'to' + str(yearMax) + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac5a81e9c3bec65",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e90aa7548e8a9b04",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_year = df['Year'].max()\n",
    "\n",
    "# Generate a list of target years, from max_year - 10 to max_year - 100, in steps of 5\n",
    "years = [max_year - i for i in range(10, 101, 5)]\n",
    "\n",
    "# Create subsets where each subset drops all rows prior to each target year\n",
    "for year in years:\n",
    "    # Create subset by dropping all rows where Year is less than the target year\n",
    "    subset = df[df['Year'] >= year]\n",
    "    output_path = '../data/final/subsets'\n",
    "    subset.to_csv(output_path + str(year) + 'to' + str(max_year)+ '.csv', index=False)\n",
    "    print(f\"Saved subset for year {year} to {output_path}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5896bd752386c1dc",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NEW APPROACH\n",
   "id": "88209faba0ee9fbf"
  },
  {
   "cell_type": "code",
   "source": "!pip install scikit-learn",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T08:04:54.597013Z",
     "start_time": "2024-05-30T08:04:29.593403Z"
    }
   },
   "id": "8616c5bc03322425",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:12:34.229979Z",
     "start_time": "2024-05-31T12:12:33.410858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_directory_merged = \"../data/merged_canton\"\n",
    "file_path_kantonsdaten_merged = os.path.join(output_directory_merged, \"Kantonsdaten_merged_full.xlsx\")\n",
    "\n",
    "df = pd.read_excel(file_path_kantonsdaten_merged)\n",
    "\n",
    "columns_to_fill = [\n",
    "    'Transferzahlungen von Kantonen an Bund',\n",
    "    'Transferzahlungen von Kantonen an andere Kantone (und Konkordate)',\n",
    "    'Transferzahlungen von Kantonen an Gemeinden (und Gemeindezweckverbände)',\n",
    "    'Transferzahlungen von Gemeinden an Kantone (und Konkordate)',\n",
    "    'Öffentliche Bildugnsausgaben',\n",
    "    'Kulturausgaben Kanton',\n",
    "    'Kulturausgaben Gemeinden',\n",
    "    'Kulturausgaben Kantone und Gemeinde',\n",
    "    'BWS Landwirtschaft, Forstwirtschaft und Fischerei',\n",
    "    'BWS Bergbau und Gewinnung von Steinen und Erden, Herstellung von Waren, Bau',\n",
    "    'BWS Energieversorgung, Wasserversorgung, Sammlung, Behandlung und Beseitigung von Abfällen, Erziehung und Unterricht, Gesundheitswesen',\n",
    "    'BWS Handel und Reparatur von Fahrzeugen, Transport, Informationsdienstleistungen und Telekommunikation, Beherbergung und Gastronomie',\n",
    "    'BWS Erbringung von Finanzdienstleistungen und Versicherungen',\n",
    "    'BWS Grundstücks- und Wohnungswesen, sonstige freiberufliche, wissenschaftliche und technische Tätigkeiten, wirtschaftlichen Dienstleistungen, Kunst, Unterhaltung und Erholung, sonstige Dienstleistungen',\n",
    "    'BWS Öffentliche Verwaltung',\n",
    "    'BWS Private Haushalte als Hersteller',\n",
    "    'BIP'\n",
    "]\n",
    "\n",
    "kantons = df['Kanton'].unique()\n",
    "\n",
    "for k in kantons:\n",
    "    df_kanton = df[df['Kanton'] == k]\n",
    "    \n",
    "    for col in columns_to_fill:\n",
    "        years = df_kanton['Jahr'].values.reshape(-1, 1)\n",
    "        values = df_kanton[col].values\n",
    "        train_mask = (years.flatten() <= 2021) & (~np.isnan(values))\n",
    "        x_train = years[train_mask].reshape(-1, 1)\n",
    "        y_train = values[train_mask]\n",
    "        \n",
    "        if len(x_train) > 1:  \n",
    "            model = LinearRegression().fit(x_train, y_train)\n",
    "            predict_mask = (years.flatten() == 2022) & (np.isnan(values))\n",
    "            if np.any(predict_mask):\n",
    "                df.loc[(df['Kanton'] == k) & (df['Jahr'] == 2022), col] = model.predict(np.array([[2022]]))[0]\n",
    "                \n",
    "df['Kanton'] = df['Kanton'].replace({'Waadt': 'VD', 'Wallis': 'VS', 'Genf': 'GE', 'Bern': 'BE', 'Freiburg': 'FR', 'Solothurn': 'SO', 'Neuenburg': 'NE', 'Jura': 'JU', 'Basel-Stadt': 'BS', 'Basel-Landschaft': 'BL', 'Aargau': 'AG', 'Zürich': 'ZH', 'Glarus': 'GL', 'Schaffhausen': 'SH', 'Appenzell A. Rh.': 'AR', 'Appenzell I. Rh.': 'AI', 'St. Gallen': 'SG', 'Graubünden': 'GR', 'Thurgau': 'TG', 'Luzern': 'LU', 'Uri': 'UR', 'Schwyz': 'SZ', 'Obwalden': 'OW', 'Nidwalden': 'NW', 'Zug': 'ZG', 'Tessin': 'TI'})\n",
    "\n",
    "df.rename(columns={\n",
    "    'Wanderungssaldo.1': 'Wanderungssaldo', \n",
    "    'Wanderungssaldo': 'Wanderungssaldo Ein-und Auswanderung',\n",
    "    'Wanderungssaldo.1': 'Wanderungssaldo Zu-und Wegzüge',\n",
    "    'Betreibungshandlungen  Pfändungsvollzüge': 'Betreibungshandlungen Pfändungsvollzüge',\n",
    "    'Transferzahlungen von Kantonen an andere Kantone (und Konkordate)': 'Transferzahlungen von Kantonen an andere Kantone',\n",
    "    'Transferzahlungen von Kantonen an Gemeinden (und Gemeindezweckverbände)': 'Transferzahlungen von Kantonen an Gemeinden',\n",
    "    'Transferzahlungen von Gemeinden an Kantone (und Konkordate)': 'Transferzahlungen von Gemeinden an Kantone',\n",
    "    'BWS Bergbau und Gewinnung von Steinen und Erden, Herstellung von Waren, Bau': 'BWS Bergbau und Gewinnung von Steinen und Erden',\n",
    "    'BWS Energieversorgung, Wasserversorgung, Sammlung, Behandlung und Beseitigung von Abfällen, Erziehung und Unterricht, Gesundheitswesen': 'BWS Versorgung, Sammlung, Entsorgung',\n",
    "    'BWS Handel und Reparatur von Fahrzeugen, Transport, Informationsdienstleistungen und Telekommunikation, Beherbergung und Gastronomie': 'BWS Transport, IT-Dienstleistung',\n",
    "    'BWS Erbringung von Finanzdienstleistungen und Versicherungen': 'BWS Finanzdienstleistungen und Versicherungen',\n",
    "    'BWS Grundstücks- und Wohnungswesen, sonstige freiberufliche, wissenschaftliche und technische Tätigkeiten, wirtschaftlichen Dienstleistungen, Kunst, Unterhaltung und Erholung, sonstige Dienstleistungen': 'BWS Wissenschaft und Kunst sowie sonstige Dienstleistungen',\n",
    "}, inplace=True)\n",
    "\n",
    "df['Betreibungshandlungen Verwertungen'] = pd.to_numeric(df['Betreibungshandlungen Verwertungen'], errors='coerce')\n",
    "\n",
    "output_path = os.path.join(output_directory_merged, \"Kantonsdaten_merged_filled.xlsx\")\n",
    "df.to_excel(output_path, index=False)"
   ],
   "id": "c93e1e51e2b079fd",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:12:34.506994Z",
     "start_time": "2024-05-31T12:12:34.231545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_directory_final = \"../data/final\"\n",
    "file_path_double_digit = os.path.join(output_directory_final, \"merged_double_digit.csv\")\n",
    "\n",
    "\n",
    "df_double_digit = pd.read_csv(file_path_double_digit)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(df_double_digit, df, how='outer', left_on=['Region', 'Year'], right_on=['Kanton', 'Jahr'])\n",
    "merged_df = merged_df[merged_df['Region'] != 0]\n",
    "merged_df.drop('Kanton', axis=1, inplace=True)\n",
    "merged_df.drop('Jahr', axis=1, inplace=True)\n",
    "\n",
    "#drop rows where column Jahr is NaN\n",
    "merged_df = merged_df.dropna(subset=['Year'])\n",
    "\n",
    "\n",
    "merged_output_path = os.path.join(output_directory_final, \"merged_complete.csv\")\n",
    "merged_df.to_csv(merged_output_path, index=False)\n"
   ],
   "id": "c82564c976d7d268",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:12:34.870813Z",
     "start_time": "2024-05-31T12:12:34.862876Z"
    }
   },
   "cell_type": "code",
   "source": "merged_df.info()\n",
   "id": "c0c019bfd2742dfe",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "725c76c91d6572ad",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
