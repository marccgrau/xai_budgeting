{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Forecasting Model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:38:05.170767Z",
     "start_time": "2024-05-24T13:37:58.978944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "!pip install optuna\n",
    "!pip install seaborn\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ],
   "execution_count": 76,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:38:05.174987Z",
     "start_time": "2024-05-24T13:38:05.172914Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "execution_count": 76,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:38:07.472336Z",
     "start_time": "2024-05-24T13:38:05.175729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/ricoschmid/git/xai_budgeting/data/final/merged_complete.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Define cutoff year for train-test split\n",
    "cutoff_year = df[\"Year\"].max() - 1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = df[df[\"Year\"] <= cutoff_year]\n",
    "test_data = df[df[\"Year\"] > cutoff_year]\n",
    "\n",
    "# Select the target column\n",
    "target_column = \"Realized\"\n",
    "\n",
    "\n",
    "\n",
    "# Convert the scaled data back to a DataFrame for easier manipulation\n",
    "scaled_train_df = pd.DataFrame(train_data, columns=train_data.columns.drop([\"Year\", \"Region\"]))\n",
    "scaled_test_df = pd.DataFrame(test_data, columns=test_data.columns.drop([\"Year\", \"Region\"]))\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(data, target_column, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    data_length = len(data)\n",
    "    \n",
    "    for i in range(data_length - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].drop(columns=[target_column]).values\n",
    "        target = data.iloc[i + sequence_length][target_column]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "        \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "X_train, y_train = create_sequences(scaled_train_df, target_column, sequence_length)\n",
    "X_test, y_test = create_sequences(scaled_test_df, target_column, sequence_length)"
   ],
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:53:14.946265Z",
     "start_time": "2024-05-24T13:38:07.473836Z"
    }
   },
   "source": [
    "import optuna\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    n_units = trial.suggest_int('n_units', 32, 128)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    input_layer = Input(shape=(sequence_length, X_train.shape[2]))\n",
    "    x = LSTM(n_units, activation='relu', return_sequences=True)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = LSTM(n_units // 2, activation='relu')(x)\n",
    "    output_layer = Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Create the Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20, timeout=1800)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ],
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train the best model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Build the best LSTM model\n",
    "best_model = Sequential()\n",
    "best_model.add(LSTM(best_params['n_units'], activation='relu', input_shape=(sequence_length, X_train.shape[2]), return_sequences=True))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(Dropout(best_params['dropout_rate']))\n",
    "best_model.add(LSTM(best_params['n_units'] // 2, activation='relu'))\n",
    "best_model.add(Dense(1))\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the best model\n",
    "train_mse = best_model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Test MSE: {test_mse}')\n",
    "\n",
    "# Save the MSE values\n",
    "mse_values = {'train_mse': train_mse, 'test_mse': test_mse}\n",
    "with open('mse_values.json', 'w') as f:\n",
    "    json.dump(mse_values, f)\n",
    "print('MSE values saved successfully')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of 'Year' in training and test data\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot for training data\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_data['Year'], train_data['Realized'], 'b', label='Train Data')\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Realized')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for test data\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(test_data['Year'], test_data['Realized'], 'r', label='Test Data')\n",
    "plt.title('Test Data')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Realized')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
